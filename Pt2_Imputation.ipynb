{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangling/math/stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# System utils\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Utils\n",
    "from utils.consts import APPRAISAL_COLS, PURCHASE_COLS\n",
    "from utils.imputation_performance import report_imputation_performance\n",
    "from utils.regression_imputation import impute_by_regression\n",
    "\n",
    "# Get local file structure\n",
    "CWD = os.getcwd()\n",
    "CWD = Path(CWD)\n",
    "DATA = CWD / \"data\"\n",
    "MODEL_DATA = DATA / \"data_for_modeling\"\n",
    "MODEL_DATA.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation of Missing Data\n",
    "\n",
    "In Part 1 I showed how many values are missing (close to 1/3rd of rows have at least one value missing). Given this, I plan to impute the missing data.\n",
    "\n",
    "## Simple Imputation\n",
    "For simplicity, I'm mostly going to use means/modes as much as possible to fill in missing data. Generally, I'll do this by making information decisions on how to subset the data. For instnce, using the mean or mode for a value ater subsetting on make/model.\n",
    "\n",
    "## (More) Complex Imputation\n",
    "After the initial imputation, I'll perform some more complex imputation using things like logistic and linear regression, primarily to fill in really commonnly missing columns:\n",
    "* trim_descrip (premium vs non-premium trim) using logistic regression\n",
    "* body using mutinomial regression\n",
    "\n",
    "A better approach would be to use iterative imputation, but the implementation in sklearn currently doesn't support the wide array of complex variable types in this dataset. For two variables, I will settle on classifier models that make use of only the naive/simple imputations for training.\n",
    "\n",
    "## Assessing Accuracy of Imputed Data\n",
    "In order to avoid information leaking (i.e. imputation capturing information about the testing data and inflating test performance), I'll define a test/train split and use only the training data for imputation and use the non-missing values in the test set to assess performance.\n",
    "\n",
    "Lastly, I'll keep track of which rows/columns have imputed values so I can evaluate how this may impacted my final value regression model(s) later.\n",
    "\n",
    "When I train regressors in future notebooks, I plan to train in two ways: with imputed data and without imputed data (i.e. dropping columns with missing data).\n",
    "\n",
    "In both cases I'll report model performance on three subsets of the test and training data separately:\n",
    "* imputed data + complete data overall\n",
    "* complete data only\n",
    "* imputed data only\n",
    "\n",
    "This will give a sense of how helpful the imputation was for model performance and whether imputed and non-imputed performance is comparable.\n",
    "\n",
    "Since there are so many rows missing just a single value, I expect that even fairly naive, somewhat ill-informed imputations (i.e. column mean or mode) will help just by increasing the amount of training data available to learn generalizable relationships.\n",
    "\n",
    "When performing model inference later, I'll also make sure to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax_data = pd.read_pickle(DATA/\"long_kmax.pkl\")\n",
    "train_data, test_data = train_test_split(kmax_data, test_size=.15)\n",
    "train_data[\"split\"] = \"train\"\n",
    "test_data[\"split\"] = \"test\"\n",
    "kmax_data = pd.concat([train_data, test_data])\n",
    "kmax_data[\"market\"] = kmax_data[\"market\"].astype(str)\n",
    "kmax_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with combined appraisal + purchase data\n",
    "Some models are rare and half are in both data sets (shown in previous dataset), and trends regarding other aspects of car features are likely to hold whether the car is being sold or purchases, so I'll use vehicle information from both sales and purchases alike to impute values in most cases.\n",
    "\n",
    "For this I converted the data to long form where each row is one end of a transaction and there is an indicator for whether it was an appraisal or purchase.\n",
    "Note: Carmax confirmed that the obfuscated make/model labels are the same for across appraisals and purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_missing_vals(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prints number of rows within specified columns with any missing values and returns\n",
    "    table summarizing which rows have missing values.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input data to check for missing values\n",
    "        subset (str): A string indicating which subset of columns to check: 'purchase,'\n",
    "            'appraisal,' or 'all'\n",
    "    \"\"\"\n",
    "    subset = \"all\"\n",
    "    if subset == \"purchase\":\n",
    "        cols = PURCHASE_COLS\n",
    "    elif subset == \"appraisal\":\n",
    "        cols = APPRAISAL_COLS\n",
    "    elif subset == \"all\":\n",
    "        cols = df.columns\n",
    "\n",
    "    rows_with_missing = df[cols].isna().any(axis=1).sum()\n",
    "    print(f\"{rows_with_missing} rows with missing ({subset} columns) data remain.\")\n",
    "\n",
    "    sns.histplot(df[cols].isna().sum(axis=1))\n",
    "    plt.title(f\"Missing Values ({subset.capitalize()} Columns)\");\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return df[cols].isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data[kmax_data[\"split\"] == \"train\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data[kmax_data[\"split\"] == \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imputation Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "def update_imputed_vals(\n",
    "        df: pd.DataFrame,\n",
    "        col: str,\n",
    "        imputed_vals: np.ndarray,\n",
    "        imputed_vals_mask: Optional[np.ndarray] = None\n",
    "        ):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        _description_\n",
    "    col : str\n",
    "        _description_\n",
    "    imputed_vals : np.ndarray\n",
    "        _description_\n",
    "    imputed_vals_mask : np.ndarray\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    imp_indicator_col = f\"{col}_imputed\"\n",
    "    has_imp_col = imp_indicator_col not in df.columns\n",
    "    no_explicit_imp_mask = isinstance(imputed_vals_mask, type(None))\n",
    "    if not no_explicit_imp_mask:\n",
    "        df[imp_indicator_col] = imputed_vals_mask  \n",
    "    elif has_imp_col and no_explicit_imp_mask:\n",
    "        imputed_vals_mask = df[imp_indicator_col]\n",
    "    elif not has_imp_col and no_explicit_imp_mask:\n",
    "        raise ValueError(\"Must pass imputed values mask if column does not already exist\")\n",
    "\n",
    "    df.loc[imputed_vals_mask, col] = imputed_vals[imputed_vals_mask]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute online appraisal flag\n",
    "Assume the purchase was offline if there is no flag.\n",
    "\n",
    "It seems unlikely that online transactions have a missing trail, it's also more common in the dataset anyways.\n",
    "\n",
    "I won't bother with imputing this iteratively (i.e. via a logistic regressor), since it's such a small number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax_data[\"online_appraisal_flag\"].value_counts(dropna=False)/len(kmax_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_appraisal_flag_mask = kmax_data[\"online_appraisal_flag\"].isna()\n",
    "imputed_appraisal_flag = np.zeros(len(kmax_data))\n",
    "report_imputation_performance(df=kmax_data, col=\"online_appraisal_flag\", imputed_vals=imputed_appraisal_flag, metric=accuracy_score)\n",
    "update_imputed_vals(kmax_data, \"online_appraisal_flag\", imputed_appraisal_flag, imputed_appraisal_flag_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax_data[\"online_appraisal_flag\"].value_counts(dropna=False)/len(kmax_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute colors\n",
    "Here I'm just going to label it as unknown since there is already an \"unknown\" label. I'm doubtful that color is meaningfully encoded by other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax_data[\"color\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Will just merge NA values into the existing \"Unknown\" column\n",
    "kmax_data[\"color\"] = kmax_data[\"color\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax_data[\"color\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Model\n",
    "I'll impute model by assuming the most common in each make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most common model for each make\n",
    "most_common_models = train_data.groupby('make')['model'].agg(pd.Series.mode)\n",
    "mode_models_by_make = most_common_models.to_dict()\n",
    "for make_key, model_val in mode_models_by_make.items():\n",
    "    if model_val and not isinstance(model_val, str):\n",
    "        mode_models_by_make[make_key] = model_val[0]\n",
    "### Fill based on most common appraisal model for each make\n",
    "imputed_model_mask = kmax_data['model'].isna()\n",
    "imputed_models = kmax_data['make'].map(mode_models_by_make)\n",
    "report_imputation_performance(df=kmax_data, col=\"model\", imputed_vals=imputed_models, metric=f1_score, plot=False)\n",
    "update_imputed_vals(kmax_data, \"model\", imputed_models, imputed_model_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very good. Model is also a very high-dimension factor. It's also a very fleeting thing (a model is far more likely to stop existing in the future than a make). I may just ignore this as a variable in the future."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how much data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Body\n",
    "\n",
    "I'll do this based primarily on model, filling anything that is still missing with the most common overall.\n",
    "I would probably be better to train a classifier based on other characteristics after initially filling with the model, but since only purchases (which I am chosing not to mostly not make use of at the moment) have missing values I won't both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill body based on the most common for that model\n",
    "body_by_models = train_data.groupby(\"model\")[\"body\"].agg(pd.Series.mode)\n",
    "body_by_models = body_by_models.to_dict()\n",
    "\n",
    "body_freqs = train_data[\"body\"].value_counts(sort=True).reset_index().rename(columns={\"index\":\"body\",\"body\":\"count\"})\n",
    "\n",
    "imputed_body_by_models = {}\n",
    "for model, body in body_by_models.items():\n",
    "    if isinstance(body, np.ndarray) or isinstance(body, str):\n",
    "        if len(body) > 1:\n",
    "            body_inds = body_freqs.index[body_freqs['body'].isin(body)]\n",
    "            body = body_freqs.iloc[min(body_inds)]['body']\n",
    "        else:\n",
    "            body = np.nan\n",
    "    else:\n",
    "        body = np.nan\n",
    "    imputed_body_by_models[model]  = body\n",
    "\n",
    "imputed_body_mask = kmax_data['body'].isna()\n",
    "imputed_body = kmax_data['body'].fillna(kmax_data['model'].map(imputed_body_by_models))\n",
    "missing_model_body = imputed_body.isna()\n",
    "imputed_body = imputed_body.fillna(kmax_data['body'].mode().values[0])\n",
    "body_train_f1, body_test_f1 = report_imputation_performance(df=kmax_data, col=\"body\", imputed_vals=imputed_body, metric=f1_score)\n",
    "update_imputed_vals(df=kmax_data, col=\"body\", imputed_vals=imputed_body, imputed_vals_mask=imputed_body_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPG City/Highway & Fuel Efficiency\n",
    "Note: MPG city/highway correlate really, really well, but they are both always missing at the same time (see cell immediately below).\n",
    "Instead, I'll use model. Where model doesn't work, I'll fill based on body average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPG\n",
    "Note: MPG city/highway correlate really, really well, but they are both always missing at the same time (see cell immediately below).\n",
    "Instead, I'll use model, body, and overall mean to help imput MPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement = accuracy_score(kmax_data[\"mpg_city\"].isna(), kmax_data[\"mpg_highway\"].isna())\n",
    "print(f\"Agreement between missing MPG city and highay values is {agreement*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPG City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Impute MPG city\n",
    "model_mpg_city = train_data.groupby(\"model\")[\"mpg_city\"].agg(pd.Series.mean).to_dict()\n",
    "body_mpg_city = train_data.groupby(\"body\")[\"mpg_city\"].agg(pd.Series.mean).to_dict()\n",
    "imputed_mpg_city = kmax_data[\"model\"].map(model_mpg_city)\n",
    "imputed_mpg_city = imputed_mpg_city.fillna(kmax_data[\"body\"].map(body_mpg_city))\n",
    "imputed_mpg_city = imputed_mpg_city.fillna(train_data['mpg_city'].mean())\n",
    "imputed_mpg_city_mask = kmax_data['mpg_city'].isna()\n",
    "mpg_city_train_r2, mpg_city_test_r2 = report_imputation_performance(df=kmax_data, col=\"mpg_city\", imputed_vals=imputed_mpg_city, metric=r2_score)\n",
    "update_imputed_vals(df=kmax_data, col=\"mpg_city\", imputed_vals=imputed_mpg_city, imputed_vals_mask=imputed_mpg_city_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPG Highay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Impute MPG highway\n",
    "model_mpg_hwy = train_data.groupby(\"model\")[\"mpg_highway\"].agg(pd.Series.mean).to_dict()\n",
    "body_mpg_hwy = train_data.groupby(\"body\")[\"mpg_highway\"].agg(pd.Series.mean).to_dict()\n",
    "imputed_mpg_hwy = kmax_data[\"model\"].map(model_mpg_hwy)\n",
    "imputed_mpg_hwy = imputed_mpg_hwy.fillna(kmax_data[\"body\"].map(body_mpg_hwy))\n",
    "imputed_mpg_hwy = imputed_mpg_hwy.fillna(train_data['mpg_highway'].mean())\n",
    "imputed_mpg_hwy_mask = kmax_data[\"mpg_highway\"].isna()\n",
    "mpg_hwy_train_r2, mpg_hwy_test_r2 = report_imputation_performance(df=kmax_data, col=\"mpg_highway\", imputed_vals=imputed_mpg_hwy, metric=r2_score)\n",
    "update_imputed_vals(kmax_data, \"mpg_highway\", imputed_mpg_hwy, imputed_mpg_hwy_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuel Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute fuel capacity\n",
    "model_fuel_capacity = train_data.groupby(\"model\")[\"fuel_capacity\"].agg(pd.Series.mean).to_dict()\n",
    "body_fuel_capacity = train_data.groupby(\"body\")[\"fuel_capacity\"].agg(pd.Series.mean).to_dict()\n",
    "mean_fuel_capacity = train_data[\"fuel_capacity\"].mean()\n",
    "imputed_fuel_capacity = kmax_data[\"body\"].map(body_fuel_capacity)\n",
    "imputed_fuel_by_body = kmax_data[\"model\"].map(model_fuel_capacity)\n",
    "imputed_fuel_capacity = imputed_fuel_capacity.fillna(kmax_data[\"model\"].map(model_fuel_capacity))\n",
    "imputed_fuel_capacity = imputed_fuel_capacity.fillna(mean_fuel_capacity)\n",
    "# Get mask\n",
    "imputed_fuel_capacity_mask = kmax_data[\"fuel_capacity\"].isna()\n",
    "# Check and update\n",
    "fuel_train_r2, fuel_test_r2 = report_imputation_performance(kmax_data, \"fuel_capacity\", imputed_fuel_capacity, r2_score)\n",
    "update_imputed_vals(kmax_data, \"fuel_capacity\", imputed_fuel_capacity, imputed_fuel_capacity_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Trim\n",
    "Fill in the most common trim type for each model (use imputed model--aka most common model for the make where not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_freqs = train_data[\"trim_descrip\"].value_counts(sort=True).reset_index().rename(columns={\"index\":\"trim_descrip\",\"trim_descrip\":\"count\"})\n",
    "### Fill trim based on the most common for that model\n",
    "_trim_by_models = train_data.groupby(\"model\")[\"trim_descrip\"].agg(pd.Series.mode)\n",
    "_trim_by_models = _trim_by_models.to_dict()\n",
    "trim_by_models = {}\n",
    "for model, trim in _trim_by_models.items():\n",
    "    if isinstance(trim, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        trim_by_models[model] = trim\n",
    "\n",
    "# Trim descrip\n",
    "imputed_trim = kmax_data['model'].map(trim_by_models)\n",
    "imputed_trim = imputed_trim.fillna(\"Not Premium\")\n",
    "imputed_trim_mask = kmax_data['trim_descrip'].isna()\n",
    "trim_train_acc, trim_test_acc = report_imputation_performance(df=kmax_data, col=\"trim_descrip\", imputed_vals=imputed_trim, metric=accuracy_score)\n",
    "update_imputed_vals(kmax_data, \"trim_descrip\", imputed_trim, imputed_trim_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_missing_vals(kmax_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression-Based Imputation Models\n",
    "Using the initially imputed values I'm now going to fit a regression model for each factor using the others to predict it.\n",
    "\n",
    "Three types of basic models will be used:\n",
    "- Linear regression (multiple linear regression)\n",
    "    - MPG City\n",
    "    - MPG Highway\n",
    "    - Fuel Capacity\n",
    "- Logistic regression\n",
    "    - Online appraisal flag\n",
    "    - Trim description\n",
    "- Multinomial Logistic Regression\n",
    "    - Body type\n",
    "\n",
    "I will not be performing any iterations -- each model will be fit based on known + imputed values from above.\n",
    "\n",
    "I'll also be making use of \"informed\" model selection for fitting these. That is, obvservations I made during EDA and/or know about cars to select features. I'm not aiming for the perfect model here, just a quickly refined set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kmax_data = kmax_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPG (Linear Regession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_mpg_city2 = impute_by_regression(\n",
    "    data=model_kmax_data,\n",
    "    target_col=\"mpg_city\",\n",
    "    model_class=LinearRegression,\n",
    "    metric=r2_score,\n",
    "    report_kwargs={\"previous_scores\": (mpg_city_train_r2, mpg_city_test_r2)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_mpg_hwy2 = impute_by_regression(\n",
    "    data=model_kmax_data,\n",
    "    target_col=\"mpg_highway\",\n",
    "    model_class=LinearRegression,\n",
    "    metric=r2_score,\n",
    "    report_kwargs={\"previous_scores\": (mpg_hwy_train_r2, mpg_hwy_test_r2)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating values\n",
    "I'm going to apply the linear model for mpg_highway, but not mpg_city.\n",
    "mpg_highway is a really nice fit\n",
    "mpg_city doesn't look much better, but since we seem to have a nice model for mpg_highway and mpg_city is so closely tied to mpg_highway, I'd like to perform the imputation iteratatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_imputed_vals(kmax_data, \"mpg_highway\", imputed_mpg_hwy2, kmax_data[\"mpg_highway_imputed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-try MPG city with updated MPG highway values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_mpg_city3 = impute_by_regression(\n",
    "    data=kmax_data,\n",
    "    target_col=\"mpg_city\",\n",
    "    model_class=LinearRegression,\n",
    "    metric=r2_score,\n",
    "    report_kwargs={\"previous_scores\": (mpg_city_train_r2, mpg_city_test_r2)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close are the regression imputed MPG city models before and after supplying updated MPG highway values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=imputed_mpg_city2[imputed_mpg_city_mask], y=imputed_mpg_city3[imputed_mpg_city_mask])\n",
    "plt.xlabel(\"Trained before updating MPG Highway\")\n",
    "plt.ylabel(\"Trained after updating MPG Highway\")\n",
    "plt.title(\"Comparison of Imputed MPG City Values Across Models\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_imputed_vals(\n",
    "    df=kmax_data,\n",
    "    col=\"mpg_city\",\n",
    "    imputed_vals=imputed_mpg_city3,\n",
    "    imputed_vals_mask=kmax_data[\"mpg_city_imputed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuel Capacity (Linear Regession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_fuel2 = impute_by_regression(\n",
    "    data=model_kmax_data,\n",
    "    target_col=\"fuel_capacity\",\n",
    "    model_class=LinearRegression,\n",
    "    metric=r2_score,\n",
    "    report_kwargs={\"previous_scores\": (fuel_train_r2, fuel_test_r2)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty obviously better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_imputed_vals(\n",
    "    df=kmax_data,\n",
    "    col=\"fuel_capacity\",\n",
    "    imputed_vals=imputed_fuel2,\n",
    "    imputed_vals_mask=kmax_data[\"fuel_capacity_imputed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Description (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_trim2 = impute_by_regression(\n",
    "    data=model_kmax_data,\n",
    "    target_col=\"trim_descrip\",\n",
    "    model_class=LogisticRegression,\n",
    "    metric=accuracy_score,\n",
    "    report_kwargs={\"previous_scores\": (trim_train_acc, trim_test_acc)},\n",
    "    model_kwargs={\"max_iter\": 5000}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agreement between naive and regression-based imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_agreement = imputed_trim == imputed_trim2\n",
    "agreement_imputed = model_agreement[imputed_trim_mask]\n",
    "print(f\"Agreement between imputed values is {agreement_imputed.sum()/imputed_trim_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing ratios of premium to not premium trim across different subsets of known and predicted alues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ratios for known values\")\n",
    "cts_trima = kmax_data[~imputed_trim_mask][\"trim_descrip\"].value_counts()\n",
    "print(cts_trima/cts_trima.sum())\n",
    "print(\"Ratios for known values (where models disagree)\")\n",
    "cts_trimb = kmax_data[~imputed_trim_mask & ~model_agreement][\"trim_descrip\"].value_counts()\n",
    "print(cts_trimb/cts_trimb.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts_imputed1a = imputed_trim[imputed_trim_mask].value_counts()\n",
    "cts_imputed1b = imputed_trim[imputed_trim_mask & ~model_agreement].value_counts()\n",
    "print(\"Ratios for simple model\")\n",
    "print(cts_imputed1a/cts_imputed1a.sum())\n",
    "print(\"Ratios for simple model (where models disagree)\")\n",
    "print(cts_imputed1b/cts_imputed1b.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts_imputed2a = pd.Series(imputed_trim2)[imputed_trim_mask].value_counts()\n",
    "cts_imputed2b = pd.Series(imputed_trim2)[imputed_trim_mask & ~model_agreement].value_counts()\n",
    "print(\"Ratios for regression model\")\n",
    "print(cts_imputed2a/cts_imputed2a.sum())\n",
    "print(\"Ratios for regression model (where models disagree)\")\n",
    "print(cts_imputed2b/cts_imputed2b.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at what other values are missing when imputed values disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_imputed_segment = kmax_data[[c for c in kmax_data.columns if \"imputed\" in c]][imputed_trim_mask]\n",
    "print(\"Missing data when both imputed values agree\")\n",
    "print(trim_imputed_segment[agreement_imputed].sum()/len(trim_imputed_segment[agreement_imputed]))\n",
    "print()\n",
    "print(\"Missing data when imputed values disagree\")\n",
    "print(trim_imputed_segment[~agreement_imputed].sum()/len(trim_imputed_segment[~agreement_imputed]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing which imputed trim values to use\n",
    "When they disagree the vehicle model is much less likely to be known, while the body is a bit more likely to be known.\n",
    "\n",
    "Further, the ratios of premium to non-premium trim returned by the regression model line up a bit better with the average of known examples when they disagree.\n",
    "\n",
    "Based on this and the overall comparable performance, I'm going to go with the regression imputed trim. This is also helpful since working with vehicle model is a bit difficult and I'd like to reduce reliance on it where possible since vehicle models get retired and introduced over time and this will impact the ability of my ML model to work in the future (even though it's a toy project it's still good to consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_imputed_vals(\n",
    "    df=kmax_data,\n",
    "    col=\"trim_descrip\",\n",
    "    imputed_vals=imputed_trim2,\n",
    "    imputed_vals_mask=kmax_data[\"trim_descrip_imputed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body (Multinomial Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_body2 = impute_by_regression(\n",
    "    data=model_kmax_data,\n",
    "    target_col=\"body\",\n",
    "    model_class=LogisticRegression,\n",
    "    metric=f1_score,\n",
    "    report_kwargs={\"previous_scores\": (body_train_f1, body_test_f1)},\n",
    "    model_kwargs={\"multi_class\": \"multinomial\", \"max_iter\": 5000, \"solver\": \"saga\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is lower than using make, which isn't surprising, since make is extremely intricately linked with body. That said, the regression model has clearly learned some good relationships.\n",
    "\n",
    "For some missing body values we imputed using the most common category because the model was either missing itself or had no available body data.\n",
    "\n",
    "It would be better to instead use the regression model to impute these edge cases instead of just labeling them all the same, so I will use the multinomial logistic regression to fill-in where that was the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agreement between simple (overall body) and MNLog body imputation methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_missing_model_body = imputed_body2[missing_model_body] == imputed_body[missing_model_body]\n",
    "missing_model_body_agreement = matching_missing_model_body.sum()/len(kmax_data)\n",
    "print(f\"The MNLog regression and mode of body generated {missing_model_body_agreement*100:.2f}% agreement for cases where the most common body per model was not known.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of simple (overall mode) imputed body values (where model body info was not availabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_body[missing_model_body].value_counts()/missing_model_body.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of MNLog imputed body values (where model body info was not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"body\": imputed_body2[missing_model_body]})['body'].value_counts()/missing_model_body.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporate new body labels for the missing model-specific info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax_data.loc[missing_model_body, \"body\"] = imputed_body2[missing_model_body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_imputed_vals = kmax_data[[c for c in kmax_data.columns if \"imputed\" in c]].any(axis=1)\n",
    "kmax_data[\"imputed\"] = any_imputed_vals\n",
    "app_data = kmax_data[\"itransaction\"] == \"appraisal\"\n",
    "kmax_data[app_data].drop(columns=[\"itransaction\"]).to_pickle(MODEL_DATA/\"appraisal.pkl\")\n",
    "kmax_data[~app_data].drop(columns=[\"itransaction\"]).to_pickle(MODEL_DATA/\"purchase.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update a few vals to make categorical/ordinal encoding easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import get_midpoint\n",
    "def merge_rare_cats(df: pd.DataFrame, cat: str, n: int = 20):\n",
    "    train = df[df[\"split\"]==\"train\"].copy()\n",
    "    make_counts = train[cat].value_counts()\n",
    "    non_rare_cats = make_counts[make_counts > n].index\n",
    "    rare_cats = ~df[cat].isin(non_rare_cats)\n",
    "    df.loc[rare_cats, cat] = \"RARE\"\n",
    "\n",
    "files = [MODEL_DATA/\"appraisal.pkl\", MODEL_DATA/\"purchase.pkl\"]\n",
    "for f in files:\n",
    "    df = pd.read_pickle(f)\n",
    "    df['mileage'] = get_midpoint(df['mileage']).rank(method=\"dense\")\n",
    "    df['value'] = get_midpoint(df['value']).rank(method=\"dense\")\n",
    "    train = df[df[\"split\"]==\"train\"].copy()\n",
    "    merge_rare_cats(df, \"make\")\n",
    "    merge_rare_cats(df, \"color\")\n",
    "    merge_rare_cats(df, \"body\")\n",
    "    merge_rare_cats(df, \"market\")\n",
    "    df.to_pickle(f)\n",
    "    df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c739df7216996af44517d475e62739a71a77ba3d210c35292fab841551565cb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
